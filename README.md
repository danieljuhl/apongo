# Apongo

Create Mongo aggregation pipelines with collection joins (using `$lookup`) for Apollo queries.

## Overview

A typical Apollo/Mongo based project will use individual GraphQL resolvers to recursively fetch fields from Mongo collections.
This approach is often sufficient, however it suffers from two major problems:

1. The recursive nature of GraphQL means that a single request can lead to many database requests.
   For example, if a request pulls in 100 results, and each of those results calls a resolver that
   requires a lookup in other table, then that single query will result in 101 database fetches.
   It's easy to see how we can quickly reach hundreds of lookups for a single Apollo query.

2. The second problem, and the most difficult one to solve, occurs when we need to fetch data from a
   primary collection, join with a secondary collection and then sort (or filter) the results based on a
   field in that second collection.
   
   A typical scenario would occur when fetching data from multiple collections to display in a table where
   the user can click a column to change the sort order.  In this case it's not sufficient to perform a simple
   Mongo `find` on the top-level collection (since the sub-fields won't be available to sort on), and as a
   result just isn't possible to use the simplified GraphQL approach of fetching the joins using resolvers.

Both of these issues can be solved by performing a *single* Mongo query that joins the tables, so that we can
then sort or filter on any field.

*Apongo* does all the heavy lifting for you:

1. It analyses the `resolveInfo` data passed to the top-level resolver in order to extract the hierarchy of
   fields that have been requested. It does this to ensure that it only performs the joins required for the
   actual query.

2. From this information it builds a __single__ Mongo aggregation pipeline that recursively performs joins
   for the other tables used in the request (using `$lookup`).

   You can then include the pipeline as part of a larger aggregation pipeline that sorts and filters the result.

## Installation

```
npm install apongo
```

You'll also need to include Apongo's types and directive in `typeDefs` and `schemaDirectives`
when calling Apollo's `makeExecutableSchema`:

The function `createPipeline` recursively analyses the requested fields and handles the creation of the
pipeline that performs the joins. To do this, it needs to know which fields are joins, and how to join them.

A custom GraphQL directive, `@apongo`, is used to specify this information directly in the types declaration.
The first thing you'll need to do is include the directive in your `schemaDirectives` when calling Apollo's `makeExecutableSchema`:

```
import { mergeTypes } from 'merge-graphql-schemas';
import { apongoDirective, apongoTypes } from 'apongo';

...

const schema = makeExecutableSchema({
  typeDefs: mergeTypes([apongoTypes, ...yourTypes]),
  resolvers,
  schemaDirectives: { apongo: apongoDirective },
});
```

## Specifying the Joins

The function `createPipeline` recursively analyses the requested fields and handles the creation of the
pipeline that performs the joins. To do this, it needs to know which fields are joins, and how to join them.
A custom GraphQL directive, `@apongo`, is used to specify this information directly in the types declaration.
You can add this directive to your type definitions to specify the joins. Here's an example:

```
type User {
  ...
  company: Company @apongo(lookup: { collection: "companies", localField: "companyId", foreignField: "_id" })
}
```

## Writing the Resolvers

In your resolver you'll need to call `createPipeline` to create the aggregation pipeline:

```
import { createPipeline } from 'apongo';

...

const users = (_, { limit = 20 }, context, resolveInfo) => new Promise((resolve, reject) => {
  // Create a pipeline to first perform the match and joins, then limit the results
  const pipeline = [
  	// Perform any initial matching that you need
    { $match: { type: 'client' } }
    
    // Include all the pipeline stages generated by Apongo to do the joins
    ...createPipeline('users', resolveInfo, context),
    
    // Filter, sort or limit the result
    { $limit: limit },
  ];

  // How you call Mongo will depend on your code base. You'll need to pass your pipeline to Mongo's aggregate
  return UsersCollection.aggregate(pipeline).next((err, res) => {
    if (err) return reject(err);
    return resolve(res);
  });
});

```

## Development Considerations

1. Remember that the directives are only used by resolvers that call `createPipeline` to create an
   aggregation pipeline. They are ignored by all other resolvers.

2. It's very important to understand that resolvers are __always__ called, even for fields which have already
   been fetched by createPipeline. In our example above, if we provide a `company` resolver for the User type
   then it will be called for each fetched user, even though it would have already been fetched by the aggregation.

   It would be very costly to allow the server to refetch all of these fields unnecessarily, so the resolvers
   need to be written to only fetch the field if it doesn't already exist in the root.
   
   Our User resolver might look like this:

```
const User = {
  // We only fetch fields that haven't been fetched by createPipeline.
  // companyId comes from the database collection, company is the result fetched via the pipeline
  company: ({ companyId, company }) => company || CompaniesCollection.findOne(companyId),
  ...
```

In the above example we simply test if `company` has already been fetched into the root object
(via the $lookup stage created by apongo), and if it hasn't we perform the lookup in the traditional way.

## The @apongo directive

### The *lookup* request

The `lookup` request accepts a number of fields:

* _collection_ : The name of the collection to join to
* _localField_ : The name of the local field used by the $lookup
* _foreignField_ : The name of foreign field used by the $lookup
* _preserveIfNull_ (Optional): Boolean to determine if the parent should should be kept if no join is found (default - true)
* _conds_ (Optional): A stringified JSON array of additional conditions use for the lookup

Sometimes your join will need extra conditions to perform the join between the two collections. Mongo's `$lookup`
command has an advanced feature that us allows use a pipeline within the lookup. Apongo uses this feature to
allow us to supply an array of extra conditions that are used when matching the collection.

Internally, this is what get added to the pipeline within the `$lookup`:

```
and: [
   { $eq: [`$${apongo.lookup.foreignField}`, '$$localField'] },
   ...JSON.parse(apongo.lookup.conds),
],
```

The `conds` needs to be a JSON array, but we have to stringify it to pass it to the directive in the types file.

Here's an example:

```
const isLimitedCompanyConds = JSON.stringify([{ $eq: ['$type', 'LIMITED'] }]).replace(/"/g, '\\"');

const types = gql`
   type User {
     ...
     limitedCompany: Company @apongo(lookup: { collection: "companies", localField: "companyId", foreignField: "_id", conds: "${isLimitedConds}" })
   }
`;
```

### The *compose* request

Apongo also provides a compose request for performing basic string composition between fields:

```
type User {
  ...
  name: String @apongo(compose: ["$profile.lastName", " ", "$profile.firstName"])
}
```

This is useful when you need to sort or filter on a composed field as part of your pipeline.

Note that Apongo takes care of replacing fields accessed by $ with the full path to that field following any joins.

### The *expr* request

This is an advanced and very rarely used feature that allows you to use the result of a Mongo aggregation expression
as the value of your field:

```
const firstEmail = JSON.stringify(
  { $arrayElemAt: [{ $map: { input: '$@path.emails', in: '$$this.address' } }, 0] },
).replace(/"/g, '\\"');


const types = gql`
   type User {
     ...
     email: String @apongo(expr: "${firstEmail}")
   }
`;
```

Wherever you need to access a field using $ you should include the token `@path`. Apongo will replace occurrences of
`@path` with the path, allowing for previous joins.

## Limitations

There's a slightly ugly problem that can arise with optional fields. Consider a GraphQL request for companies and their associated owner's name.

```
type Company {
  ...
  owner: Person @apongo(lookup: { collection: "people", localField: "ownerId", foreignField: "_id" })
}

type Person {
  _id: ID!
  ...  
  name: String @apongo(compose: ["$profile.lastName", " ", "$profile.firstName"])
}
```

Note that the `Company` type _may_ have a known owner, and that the `Person` type uses the `@apongo` compose directive to create the full name field by concatenating other fields.
 
When fetching the company, the join may fail to return a owner if there isn't one. That's fine.
However, if we requested the owner's name as part of the requesnt then the pipeline will continue to produce a result
for the owner object, resulting in an incomplete object where _there shouldn't be one at all_:

```
{
  ...other company fields...
   
  client: {Â name: null }
}
```

Later, when the Company's `owner` resolver is called, the above (incomplete) owner object _does_ exist, even though it _shouldn't_. In this case, we want to return null from the resolver:

```
const Company = {
  // We only fetch fields that haven't been fetched by createPipeline.
  owner: ({ ownerId, owner }) => owner ? (owner._id ? owner : null) : UsersCollection.findOne({ _id: ownerId }),
```

If there's an owner object without an `_id` then we know we should actually return null, otherwise we return the pre-fetched owner, or fetch from the database.

Note that it would have been possible to for Apongo to remove this result completely, but it leaving it does have
the advantage of allowing the resolver to detect that a lookup was actually attempted, so we can just return null 
without another call to the database.

What happens when we don't have a directive which adds a new field and there's no document found?
In that case the resolver actually does receive null for the joined field, and it can't know that an attempt
was made to do the join. In this case we'll have to __unnecessarily__ call the database (which will again return `null`).
Such is life.

## Recipes

### Pagination

Displaying a table of paginated data across multiple collections is likely to be one of the major uses for Apongo.
Typically when displaying paginated data we need to supply the Apollo client with both the data to display,
and also the total number of results so that the total number of pages can be displayed on the UI.

By enhancing the aggregation pipeline we can do this quite easily:

```
const paginatedUsers = (_, { limit = 20, offset = 0 }, context, resolveInfo) => new Promise((resolve, reject) => {
  // Create a main pipeline to first perform the match and joins
  const pipeline = [
    // Perform any initial matching that you need
    { $match: { type: 'client' } }
    
    // Include all the pipeline stages generated by Apongo to do the joins
    ...(resolveInfo ? createPipeline('users', resolveInfo, context) : []),
  ];

  // Create a separate pagination pipeline that will sort, skip and limit the results
  const pageSize = Math.min(limit, MAX_PAGE_SIZE);
  const paginatedPipeline = [
    { $sort: [{ id: 'name', desc: 1 }] },
    { $skip: offset },
    { $limit: pageSize },
  ];

  // Split the main pipeline into two facets, one to return the paginated result using the pipeline
  // above, and the other to get the total count of matched documents
  pipeline.push(
    {
      $facet: {
        users: paginatedPipeline,
        count: [
          { $group: { _id: null, count: { $sum: 1 } } },
        ],
      },
    },
  );

  return UsersCollection.aggregate(pipeline).next((err, res) => {
    if (err) return reject(err);
    const { users, count } = res;
    return resolve({ users, count: count.length === 0 ? 0 : count[0].count });
  });
});
```

### Meteor Compatibility

Meteor doesn't natively provide access to Mongo's aggregation command. Fortunately this oversight can be
rectified with a this [tiny meteor package](https://github.com/meteorhacks/meteor-aggregate).
