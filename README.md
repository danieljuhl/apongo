# Apongo

Create Mongo aggregation pipelines with collection joins for Apollo queries.

## Overview

A typical Apollo/Mongo based project will use individual GraphQL resolvers to recursively fetch fields from Mongo collections.
This approach is often sufficient, however it suffers from two major problems.

1. The recursive nature of GraphQL means that a single request can lead to many database requests.
   For example, if a request pulls in 100 results, and each of those results calls a resolver that
   requires a lookup in other table, then that single query will result in 101 database fetches.
   It's easy to see how we can quickly reach hundreds of lookups for a single Apollo query.

2. The second problem, and the most difficult one to solve, occurs when we need to fetch data from a
   primary collection, join with a secondary collection and then sort (or filter) the results based on a
   field in that second collection.
   
   A typical scenario would occur when fetching data from multiple collections to display in a table where
   the user can click a column to change the sort order.  In this case it's not possible to perform a simple
   Mongo `find` on the top-level collection (since the sub-fields won't be available to sort on), and as a
   result just isn't possible to use the simplified GraphQL approach of fetching the joins using simple resolvers.

Both of these issues can be solved by performing a *single* Mongo query that joins the tables, so that we can
then sort or filter on any field.

*Apongo* does all the heavy lifting for you:

1. It analyses the `resolveInfo` data passed to the top-level resolver in order to extract the hierarchy of
   fields that have been requested. It does this to ensure that it only performs the joins required for the
   actual query.

2. From this information it builds a __single__ Mongo aggregation pipeline that recursively performs joins
   for the other tables used in the request (using `$lookup`).

You can then include the pipeline as part of a larger aggregation pipeline that sorts and filters the result.

## Installation

```
npm install apongo
```

## Specifying the Joins

The function `createPipeline` recursively analyses the requested fields and handles the creation of the
pipeline that performs the joins. To do this, it needs to know which fields are joins, and how to join them.

A custom GraphQL directive, `@apongo`, is used to specify this information directly in the types declaration.
The first thing you'll need to do is include the directive in your `schemaDirectives` when calling Apollo's `makeExecutableSchema`:

```
import { joinerDirective } from 'apongo';

...

const schema = makeExecutableSchema({
  typeDefs,
  resolvers,
  schemaDirectives: { apongo: apongoDirective },
});
```

Then you can add this directive to your type definitions to specify the joins. Here's an example:

```
type User {
  ...
  company: Company @apongo(lookup: { collection: "companies", localField: "companyId", foreignField: "_id" })
}
```

## Writing the Resolvers

In your resolver you'll need to call `createPipeline` to create the aggregation pipeline:

```
import { createPipeline } from 'apongo';

...

const users = (_, { limit = 20 }, context, resolveInfo) => new Promise((resolve, reject) => {
  // Create a pipeline to first perform the match and joins, then limit the results
  const pipeline = [
  	// Perform any initial matching that you need
    { $match: { type: 'client' } }
    
    // Include all the pipeline stages generated by Apongo to do the joins
    ...(resolveInfo ? createPipeline('users', resolveInfo, context) : []),
    
	// Filter, sort or limit the result
    { $limit: limit },
  ];

  // How you call Mongo will depend on your code base. You'll need to pass your pipeline to Mongo's aggregate
  return UsersCollection.aggregate(pipeline).next((err, res) => {
    if (err) return reject(err);
    return resolve(res);
  });
});

```

## Development Considerations

1. Don't forget to add @apongo directives where necessary.

2. Remember that the directives are only used by endpoints that call `createPipeline` to create an
   aggregation pipeline. They are ignored by all other resolvers.

3. It's very important to understand that resolvers are __always__ called, even for fields which have already
   been fetched by createPipeline. In our example above, if we provide a `company` resolver for the User type
   then it will be called for each fetched user, even though it would have already been fetched by the aggregation.

   It would be very costly to allow the server to refetch all of these fields unnecessarily, so the resolvers
   need to be written to only fetch the field if it doesn't already exist in the root.
   
   Our User resolver might look like this:

```
const User = {
  // We only fetch fields that haven't been fetched by createPipeline.
  // companyId comes from the database collection, company is the result fetched via the pipeline
  company: ({ companyId, company }) => company || CompaniesCollection.findOne(companyId),
  ...
```

In the above example we simply test if `company` has already been fetched into the root object
(via the $lookup stage created by apongo), and if it hasn't we perform the lookup in the traditional way.

## The @apongo directive

### The *lookup* request

We've seen a basic example of the `@apongo` directive. The `lookup` request accepts a number of fields:

* _collection_ : The name of the collection to join to
* _localField_ : The name of the local field used by the $lookup
* _foreignField_ : The name of foreign field used by the $lookup
* _conds_: (Optional): A stringified JSON array of additional conditions use for the lookup

Sometimes your join will need extra condition to perform the join between the two collections. Mongo's `$lookup`
command has an advanced feature that us allows use a pipeline within the join. We can supply an array of extra
conditions that's used to match the sub-collection.

Internally, this is what get added to the pipeline:

```
and: [
   { $eq: [`$${apongo.lookup.foreignField}`, '$$localField'] },
   ...JSON.parse(apongo.lookup.conds),
],
```

The `conds` needs to be a JSON array, but we have to stringify it to pass it to the directive in the types file.

Here's an example:

```
const isLimitedCompanyConds = JSON.stringify([{ $eq: ['$type', 'LIMITED'] }]).replace(/"/g, '\\"');

const types = gql`
   type User {
     ...
     company: Company @apongo(lookup: { collection: "companies", localField: "companyId", foreignField: "_id", conds: "${isLimitedConds}" })
   }
`;
```

### The *compose* request

Apongo also provides a compose request for performing basic string composition:

```
type User {
  ...
  name: String @apongo(compose: ["$profile.lastName", " ", "$profile.firstName"])
}
```

This is useful when you need to sort or filter on a composed field as part of your pipeline.

### The *expr* request

This is a rarely used feature that allows you to use a Mongo aggregation expression as the result of your field:

```
const firstEmail = JSON.stringify(
  { $arrayElemAt: [{ $map: { input: '$@path.emails', in: '$$this.address' } }, 0] },
).replace(/"/g, '\\"');


const types = gql`
   type User {
     ...
     email: String @apongo(expr: "${firstEmail}")
   }
`;
```

Apongo will replace occurrences of `@path` with ancestor the path. If the above code were to use `$emails` then the expression would alone work for the root collection, not for the sub-collection that have ben joined.

## Limitations

There's a slightly ugly problem that can arise with optional fields. Consider a GraphQL request for companies and their associated owner's name.

```
type Company {
  ...
  owner: Person @apongo(lookup: { collection: "people", localField: "ownerId", foreignField: "_id" })
}

type Person {
  _id: ID!
  ...  
  name: String @apongo(compose: ["$profile.lastName", " ", "$profile.firstName"])
}
```

The `Company` type _may_ have a known owner, and the `Person` type uses the `@apongo` compose directive to create the full name field by concatenating other fields.
 
The join may therefore fail to return a owner if there isn't one. That's fine. However, if we requested the owner's
name then the pipeline will continue to produce a result for the owner object, resulting in an incomplete object
where _there shouldn't be one at all_:

```
{
  ...other company fields...
   
  client: {Â name: null }
}
```

Later, when the Company's `owner` resolver is called, the above (incomplete) owner object _does_ exist, even though it _shouldn't_. In this case, we __need__ to return null from the resolver:

```
const Company = {
  // We only fetch fields that haven't been fetched by createPipeline.
  owner: ({ ownerId, owner }) => owner ? (owner._id ? owner : null) : UsersCollection.findOne({ _id: ownerId }),
```

If there's an owner object without an `_id` then we know we should actually return null, otherwise we return the pre-fetched owner, or fetch from the database.

What happens when we don't have a directive which adds a new field and there's no document returned at all?
In that case the resolver can't know that we've made an attempt to do the join, and we'll have to __unnecessarily__
call the database (which will again return `null`). Such is life.

## Recipes

### Pagination

Displaying a table of paginated data across multiple collections is likely to be one of the major uses for Apongo.
Typically when displaying paginated data we need to supply the Apollo client with both the data to display,
and also the total number of results so that the total number of pages can be displayed on the UI.

By enhancing the aggregation pipeline we can do this quite easily:

```
const paginatedUsers = (_, { limit = 20, offset = 0 }, context, resolveInfo) => new Promise((resolve, reject) => {
  // Create a main pipeline to first perform the match and joins
  const pipeline = [
    // Perform any initial matching that you need
    { $match: { type: 'client' } }
    
	// Include all the pipeline stages generated by Apongo to do the joins
    ...(resolveInfo ? createPipeline('users', resolveInfo, context) : []),
  ];

  // Create a pagination pipeline that will sort, skip and limit the results
  const pageSize = Math.min(limit, MAX_PAGE_SIZE);
  const paginatedPipeline = [
    { $sort: [{ id: 'name', desc: 1 }] },
    { $skip: offset },
    { $limit: pageSize },
  ];

  // Split the main pipeline into two facets, one to sort and limit by page size, and the other to
  // get the total count of matched documents
  pipeline.push(
    {
      $facet: {
        users: paginatedPipeline,
        count: [
          { $group: { _id: null, count: { $sum: 1 } } },
        ],
      },
    },
  );

  return UsersCollection.aggregate(pipeline).next((err, res) => {
    if (err) return reject(err);
    const { users, count } = res;
    return resolve({ users, count: count.length === 0 ? 0 : count[0].count });
  });
});
```